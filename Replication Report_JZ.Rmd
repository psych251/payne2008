---
title: "Replication of The Study by Payne et al. (2008, Psychological Science)"
author: "Jinxiao Zhang (jinx.zhang@stanford.edu)"
date: "`r format(Sys.time())`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---
  
*Links to*
[github repo](https://github.com/tepzhang/payne2008), [osf preregistration](https://osf.io/mj7nd/), [session 1 experiment](https://stanforduniversity.qualtrics.com/jfe/form/SV_0jLDw3H3QLV7Usl), [session 2 experiment](https://stanforduniversity.qualtrics.com/jfe/form/SV_77jw7DtC0C3R1Zz), and [the original paper](https://github.com/tepzhang/payne2008/blob/master/original_paper/Payne%20et%20al.%202008%20PS.pdf)


<!-- R pub: http://rpubs.com/jxzhang/psych251_report  -->

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

##Introduction

  This study by Payne and colleagues is one of the earliest and most cited studies on the effect of sleep on emotional memory. Participants in 4 conditions viewed neutral and negative scences before a recognition test. Participants in the morning 30-min condition and the evening 30-min condition were tested 30 minutes after viewing the scences in the morning and in the evening respectively. Participants in the wake-delay condition viewed the scences in the morning and were tested 12 hours later in the evening with no daytime sleep.  Participants in the sleep-delay condition viewed the scences in the evening and were tested 12 hours later in the morning with night-time sleep. Participants in the morning 30-min condition and the evening 30-min conditions did not differ in their recognition of neutral or negative scenes, suggesting no circadian effect on the memory. However, recognition of negative objects were better recognized in the sleep-delay condition than the wake-delay condition. The recognition of neutral objects or background did not differ between the sleep-delay condition and the wake-delay condition. As there is no circadian effect, this replication study only replicate their finding that sleep specifically enhances memory of emotional objects as compared with an equal time duration of wakefulness.
  
  This replication study will recruit 48 participants to be randomly assigned to either a wake-delay condition or a sleep-delay condition. Participants in the wake-delay condition view the stimuli between 8 and 10 am and are tested 12 hours later between 8 and 10 pm. Participants in the sleep-delay condition view the stimuli between 8 and 10 pm and are tested between 8 and 10 am. In the learning phase, participants view a set of scences with either a nagetive object (e.g., a car accident) or a neutral object (e.g., an intact car) on a neutral background (e.g., a street). Twelve hours later in the test phase, they are presented with a series of objects and backgrounds one at a time. Some of the objects and backgrounds are identical to those in the learning phase, some of the objects and backgrounds are similar but not identical to those in the learning phase, and others are new objects and backgrounds. For each object or background, participants indicate whether it is identical, similar, or new to those learned 12 hours ago.

##Methods
###Power Analysis

The original sample size is n=48 with 24 in the wake-delay condition and 24 in the sleep-delay condition. The major result in Payne et al. (2018) is a 2 (condition: sleep, wake) × 2 (valence: negative, neutral) within-between interaction on recognition of objects, *F*(1, 46) = 11.5, *p* = .001, $\eta^2_{p}$ = .20. To detect such effect size ($\eta^2_{p}$ = .20 or equivalently f = .5) with 80% power and an assumed medium correlation = .3 among repeated measures, a total sample size = 14 is required. To achieve 90% and 95% poewr, a total sample size = 18 and 22 is required respectively.

###Planned Sample

Although the power analysis showed that a smaller sample size is needed than the orginal study, in order to strictly replicate the original study, this replication project will recruit 24 participants in the wake-delay condition and 24 participants in the sleep-delay conditions.

###Materials

"The scenes portrayed negative arousing or neutral objects placed on plausible neutral backgrounds. For each of 64 scenes (e.g., a car on a street), we created eight different versions by placing each of two similar neutral objects (e.g., two images of a car) and each of two related negative objects (e.g., two images of a car accident) on each of two plausible neutral backgrounds (e.g., two images of a street). An additional 32 scenes served as lures on a recognition memory test (Fig. 1). Participants in a previous study had rated the objects and backgrounds for valence and arousal, using 7-point scales (Kensinger, Garoff-Eaton, & Schacter, 2006). All negative objects had received arousal ratings of 5 to 7 (with high scores signifying an exciting or arousing image) and valence ratings lower than 3 (with low scores signifying a negative image). All neutral items (objects and backgrounds) had been rated as nonarousing (arousal values lower than 4) and neutral (valence ratings between 3 and 5)." 

This was followed precisely.

![**Fig. 1.** "Examples of the scenes presented to subjects. Eight versions of each scene were created by combining each of four similar objects (two neutral objects, two negative and arousing emotional objects) with each of two plausible neutral backgrounds. In this example, the two neutral central objects are cars, and the two negative central objects are cars damaged in an accident; the neutral backgrounds are street scenes. Two of the eight versions of the completed scene are shown."](http://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/pssa/2008/pssa_19_8/j.1467-9280.2008.02157.x/20160829/images/medium/10.1111_j.1467-9280.2008.02157.x-fig1.gif)

###Procedure	
"Participants studied a set of 64 scenes (32 with a neutral object and 32 with a negative object, all on neutral backgrounds) for 5 s each, and then indicated on a 7-point scale whether they would approach or move away from the scene if they encountered it in real life. This task was used to maximize encoding.

After the delay period, participants performed an unexpected, self-paced recognition task. During this task, objects and backgrounds were presented separately and one at a time. Some of these objects and backgrounds were identical to the scene components that had been studied (e.g., the same car accident), others were the alternate version of the object or background and therefore shared the same verbal label but differed in specific visual details (e.g., a similar car accident), and others were objects or backgrounds that had not been studied (new). Participants never saw both the same and the similar version of an item at test. Each object or background was presented with a question (e.g., "Did you see a monkey?"). If the answer to the question was "yes," participants pressed one button to indicate that the object or background was an exact match to a studied component ("same") or a second button to indicate that it was not an exact match ("similar"). If the answer to the question was "no," they pressed a third button.

The recognition task included 32 same objects (16 negative, 16 neutral), 32 similar objects (16 negative, 16 neutral), 32 new objects (16 negative, 16 neutral), 32 same backgrounds (16 previously shown with a negative object, 16 previously shown with a neutral object), 32 similar backgrounds (16 previously shown with a negative object, 16 previously shown with a neutral object), and 32 new backgrounds." 

This was followed precisely. For the current purpose, the experiment sessions before and after the delay period were referred as Session 1 and Session 2 respectively.

###Analysis Plan

"We scored a response as specific recognition of visual details when a subject correctly responded "same" to a *same* item, but as general recognition without specific details when a subject responded "similar" to a *same* item. Because "similar" responses were constrained by the number of "same" responses (i.e., subjects responded "similar" only when they did not remember the visual details), we computed the general recognition score as the proportion of "similar" responses after exclusion of "same" responses (similar/[1- same])." "Specific and general recognition scores were computed separately for central objects (negative or neutral) and for the peripheral neutral backgrounds (studied with either a negative or a neutral object)." 

To replicate the results, a 2 (condition: sleep, wake) × 2 (valence: negative, neutral) × 2 (scene component: object, background) mixed ANOVA was applied on general recognition. And planned follow-up 2 (condition: sleep, wake) × 2 (valence: negative, neutral) mixed ANOVA were applied on the recognition of objects and backgrounds separately.

**Clarify key analysis of interest here** The key analysis in this replication is the 2 (condition: sleep, wake) × 2 (valence: negative, neutral) mixed ANOVA on the general recognition of objects. The replication target is the 2 (condition: sleep, wake) × 2 (valence: negative, neutral) interaction, *F*(1, 46) = 11.5, *p* = .001, $\eta^2_{p}$ = .20.

###Differences from Original Study

This replication project differs from the original study in three major ways. First, the original experiments were run in a laboratory while the current replication experiments were run online. Second, participants in the original study were college students while participants in the current replication study were workers on Amazon Mechanical Turk (MTurk). Third, participants in the original study were randomly assigned to a condition (sleep or wake) whereas participants in the current replication self-assigned themselves to a condition in the online experiment. These differences were not anticipated to make a difference based on the claim in the original paper that "memory for a negative scene develops differentially across time delays containing sleep and wake, with sleep selectively consolidating those aspects of memory that are of greatest value to the organism."

<!-- 

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

-->

#### Actual Sample

  Fourty-five participants completed Session 1, and only 25 of them finished Session 2. Two more participants were excluded because they finished Session 2 within one hour after Session 1, leaving a final sample of 23 participants (43.5% female; age = 39.4 $\pm$ 10.9 years). 91.3% of the sample has finished or attended college education. In the final sample, 14 were in the sleep condition, and 9 were in the wake condition.

#### Differences from pre-data collection methods plan
  
  Due to a slow data collection speed and a high dropout rate, the final sample has only 23 participants, which is only about a half of the planned sample size (*n* = 48).


## Results

### Data preparation

Participation time in both sessions were checked first. Participants who did the experiment out outside the study window (8-10 am and 8-10 pm) and participants who completed Session 2 not approximately 12 hours after Session 1 (10-14 hours) were excluded.

In Session 2, participants gave a response as "same", "similar", or "new" to each stimulus. Data were gathered into a long form so that one trial is in a row and a "response" variable were created. Labels of "component" (object or background), "valence" (neutral or negative), and "type" (same, similar or new) were specified for each trial. Following the original study, a "specific recognition" score was calculated as the proportion of "same" responses to all "same" stimuli for each participant. A "general recognition" score was calculated as the proportion of "similar" responses to "same" stimuli after exlusion of "same" responses for each participant. In addition, an "all recognition" score was calculated as the proportion of ("same" + "similar") responses to "same" stimuli.
	
```{r include = F}
###Data Preparation
# dowload from Qualtrics

####Load Relevant Libraries and Functions
library(qualtRics)
library(tidyverse)
library(ggplot2)
library(cowplot)
library(stringr)
library(lubridate)# date and time
library(ez) # for repeated-measure ANOVA
#library(lme4)
#library(lmerTest) #optional: show p-value in lmer summary
library(scales) # print percentage
library(knitr)

setwd("C:/Users/tepzh/Dropbox/Stanford/PSYCH251 experimental methods/replication project/payne2008")
```

```{r }
####Import data
#session 1 data
data_s1 <- readSurvey("data/PSYCH251+Session+1.csv")
#wake group data
data_s2_wake <- readSurvey("data/PSYCH251+Session+2_sleep.csv")
#sleep group data
data_s2_sleep <- readSurvey("data/PSYCH251+Session+2_wake.csv")
#combine the two raw datasets
data_s2 <- rbind(data_s2_wake,data_s2_sleep)

#### Data exclusion / filtering
# clean s1 data
data_s1_clean <- data_s1 %>% 
  select(-contains("Q34"), -contains("Q35"), -contains("Q19"), -contains("Q24"), -contains("prac")) %>% 
  filter(!is.na(am_or_pm))  
# final data collection started on 2018/12/12
data_s1_clean <- data_s1_clean[mdy_hm(data_s1_clean$StartDate) >= ymd("2018-12-12"),]
# mark S1 variable names
names(data_s1_clean) <- str_c("s1_", names(data_s1_clean))

# clean s2 data
data_s2_clean <- data_s2 %>% 
  select(-contains("Q34"), -contains("Q35"), -contains("Q31"), -contains("Q42"), -contains("Q43"), -starts_with("Recipient"), -contains("s2p")) %>% 
  filter(!is.na("sleep_log#4_1"), 
         code_s1 != "999999", code_s1 != "99999", code_s1 != "888888")
# final data collection started on 2018/12/12
data_s2_clean <- data_s2_clean[mdy_hm(data_s2_clean$StartDate) >= ymd("2018-12-12"),]
# categorize participants into sleep-condition and wake-condition
data_s2_clean <- data_s2_clean %>% 
  mutate(time = mdy_hm(StartDate), condition = ifelse(hour(time) >= 5 & hour(time) <= 12, "sleep", "wake")) # considering time difference, the sleep group (across the US) starts Session 2 between 5 am and 12 pm 

# join s1 and s2 data
data_s1s2 <- left_join(data_s2_clean, data_s1_clean, by = c("code_s1" = "s1_code"))
# calculate the s1-s2 time gap
data_s1s2 <- data_s1s2 %>% 
  mutate(time_gap = as.numeric(mdy_hm(StartDate) - mdy_hm(s1_StartDate), units = "hours") ) # time gap in hours
# filter out participants who did not have S1 and S2 about 12 hours apart
data_s1s2 <- data_s1s2 %>% 
  filter(time_gap >10 & time_gap  < 14)
# check the condition assignment (is it consistent with reported am_or_pm during Session 1)
table(data_s1s2$condition, data_s1s2$s1_am_or_pm)

# distribution of S2 time of the day
ggplot(data_s1s2, aes(x = hour(time), fill = condition))+
  geom_histogram(color = 'black') +
  labs(title = "Distribution of the time of Session 2 (in Pacific Time)", x = "time of the day (hour)")+
  xlim(0, 24) + scale_y_continuous(breaks=seq(0, 10, 1))

# distribition of the time gap between S1 and S2
ggplot(data_s1s2, aes(x = time_gap))+
  geom_histogram(color = 'black', fill = 'white') +
  labs(title = "Distribution of time gap between Session 1 and Session 2", x = "duration (hours)")+
  xlim(11, 14)
```

```{r}
#### Prepare data for analysis - create columns etc.
# temp1: data with Q1 gathered (Q1: did you see this object/background?)
data <- data_s1s2 %>% 
  gather(item, s2_q1, contains("s2_q1")) %>% 
  mutate(item = substr(item, 1, str_length(item)-3))

# create item_number and rearrange the dataframe
data <- data %>% 
  mutate(item_number = as.numeric(str_sub(item, 1, str_length(item)-3))) %>%
  arrange(code_s1, item_number)


# read the stimuli list
item_list <- read_csv('stimuli list/questions_list.csv')
# merge the item list to the data frame
data_joined <-  left_join(data, item_list, by = "item_number") 
# rearrange
data_joined <- data_joined %>% arrange(code_s1, item_number) 

## summerize into individual-wise data
data_ind <- data_joined %>% 
  # filter(type == "same") %>%  #only "same" trials were processed, one participant has 64 trials
  group_by(code_s1, condition, type, component, valence, s2_q1) %>% 
  mutate(count_ans = n()) #the number of all answers separately (same, similar, new)
data_ind <- data_ind %>% 
  # filter(type == "same") %>% 
  group_by(code_s1, condition, type, component, valence) %>% 
  mutate(count_all = n()) #the number of questions
# collapse to individual level
data_ind <- data_ind %>% 
  group_by(code_s1, condition, type, component, valence, s2_q1) %>% 
  summarise(count_ans = mean(count_ans, na.rm = T),
            count_all = mean(count_all, na.rm = T)) %>% 
  #filter(!is.na(s2_q1)) %>% #s2_q1 has to have a value
  spread(s2_q1, count_ans) #spread s2_q1

#replace na with 0
data_ind[is.na(data_ind)] <-  0 
#calculate specific and general recognition rate
data_ind <-  data_ind %>% 
  mutate(specific_recog = Identical/count_all, #specific recognition
         general_recog = Similar/(count_all-Identical) #general recognition
         ) 

# order the level of condition
data_ind$condition <- factor(data_ind$condition, levels=c('wake', 'sleep'), ordered=TRUE)
data_ind$component <- factor(data_ind$component, levels=c('object', 'background'), ordered=TRUE)

# show the head of the individual-wise data
kable(head(data_ind, 11), digit = 2)
```

###Confirmatory analysis
```{r}
## 3-way anova: test on the 3-way interaction
anova_3way = ezANOVA(
    data = data_ind %>% filter(type == "same"),
    dv = .(general_recog),
    wid = .(code_s1), 
    within = .(component, valence),
    between = .(condition)
)
#Show the ANOVA & assumption tests.
print(anova_3way)
```
A 2 (condition: sleep, wake) × 2 (valence: negative, neutral) × 2 (scene component: object, background) mixed ANOVA on general recognition did not show a 3-way condition by valence by scene component interaction, *F*(1, 21) = .83, *p* = .371, $\eta^2_{p}$ = .005. It only showed a component × valence interaction, *F*(1, 21) = 12.13, *p* = .002, $\eta^2_{p}$ = .063.

```{r}
## 2-way anova: test on the key 2-way interaction on object recognition
anova_2way_obj = ezANOVA(
    data = data_ind %>% filter(type == "same", component == "object"),
    dv = .(general_recog),
    wid = .(code_s1), 
    within = .(valence),
    between = .(condition)
)
print(anova_2way_obj)
```
A planned 2 (condition: sleep, wake) × 2 (valence: negative, neutral) mixed ANOVA on the general recognition of objects did not showed a significant condition by valence interaction, *F*(1, 21) = 2.34, *p* = .141, $\eta^2_{p}$ = .025. There was only a marginal effect of valence, *F*(1, 21) = 3.88, *p* = .062, $\eta^2_{p}$ = .040.

```{r}
## 2-way anova: test on the 2-way interaction on background
anova_2way_bgd = ezANOVA(
    data = data_ind %>% filter(type == "same", component == "background"),
    dv = .(general_recog),
    wid = .(code_s1), 
    within = .(valence),
    between = .(condition)
)
print(anova_2way_bgd)
```
And a 2 (condition: sleep, wake) × 2 (valence: negative, neutral) mixed ANOVA on the recognition of backgrounds showed no interaction effect, *F*(1, 21) = 0.95, *p* = .761, $\eta^2_{p}$ = .001. However, there was a main effect of valence, *F*(1, 21) = 12.62, *p* = .002, $\eta^2_{p}$ = .091.

###Data plots
```{r}
# summarize data for plotting
data_summary <- data_ind %>% 
  filter(type == "same") %>% 
  group_by(condition, component,valence) %>% 
  summarise(mean_spe_recog = mean(specific_recog, na.rm = T), 
            sd_spe_recog = sd(specific_recog, na.rm = TRUE), 
            se_spe_recog = sd_spe_recog/sqrt(n()), 
            ci_spe_recog =  1.96*se_spe_recog,# qt(0.975,df=length(specific_recog)-1)
            mean_gen_recog = mean(general_recog, na.rm = T), 
            sd_gen_recog = sd(general_recog, na.rm = TRUE), 
            se_gen_recog = sd_gen_recog/sqrt(n()), 
            ci_gen_recog =  1.96*se_gen_recog, # qt(0.975,df=length(general_recog)-1)
            # mean_all_recog = mean(all_recog, na.rm = T), 
            # sd_all_recog = sd(all_recog, na.rm = TRUE), 
            # se_all_recog = sd_all_recog/sqrt(n()), 
            # ci_all_recog =  1.96*se_all_recog, # qt(0.975,df=length(general_recog)-1)
            count = n())
# order the level of condition
# data_summary$condition <- factor(data_summary$condition, levels=c('wake', 'sleep'), ordered=TRUE)
# data_summary$component <- factor(data_summary$component, levels=c('object', 'background'), ordered=TRUE)

kable(head(data_summary, 8), digits = 2)
```

The results of general recognition of the two groups (wake and sleep) is in the subplot A blow.
```{r}
# plot general recognition
p1 <- ggplot(data_summary , aes(x = condition, y = mean_gen_recog, fill = component))+
  geom_bar(color = "black", stat = "identity", position=position_dodge(), width=0.5) + 
  geom_errorbar(aes(ymin=mean_gen_recog - ci_gen_recog, ymax=mean_gen_recog + ci_gen_recog), position=position_dodge(.5), width=.2, size=1) +
  #geom_point(data = data_ind, aes(x = condition, y = general_recog, fill = component), position=position_dodge(.5), color = "darkred", alpha = .5)+
  facet_grid(. ~ valence)+
  labs(title = "General recognition")+
  scale_fill_grey()+ ggthemes::theme_few()

# plot specific recognition
p2 <- ggplot(data_summary , aes(x = condition, y = mean_spe_recog, fill = component))+
  geom_bar(color = "black", stat = "identity", position=position_dodge(), width=0.5) + 
  geom_errorbar(aes(ymin=mean_spe_recog - ci_spe_recog, ymax=mean_spe_recog + ci_spe_recog), position=position_dodge(.5), width=.2, size=1) +
  #geom_point(data = data_ind, aes(x = condition, y = general_recog, fill = component), position=position_dodge(.5), color = "darkred", alpha = .5)+
  facet_grid(. ~ valence)+
  labs(title = "Specific recognition")+
  scale_fill_grey()+ ggthemes::theme_few()

plot_grid(p1, p2, ncol = 1, labels = c('A', 'B'))
```

And the result figure in the original study is Fig. 3a and 3b.

![**Fig. 3.** "Mean recognition memory for objects and backgrounds in the three delay conditions: 30 min, wake delay, and sleep delay. Separate graphs show results for (a) general recognition for negative scenes, (b) general recognition for neutral scenes, (c) specific recognition for negative scenes, and (d) specific recognition for neutral scenes. The dotted arrows highlight the difference in memory for negative arousing objects between the 30-min condition and the 12-hr sleep condition."](original_paper/Fig 3.gif)


###Exploratory analyses
```{r}
## test on the 2-way interaction on specific recognition of object
anova_2way_spe_recog = ezANOVA(
    data = data_ind %>% filter(type == "same", component == "object"),
    dv = .(specific_recog),
    wid = .(code_s1), 
    within = .(valence),
    between = .(condition)
)
print(anova_2way_spe_recog)

## test on the 2-way interaction on specific recognition of background
anova_2way_spe_recog_bgd = ezANOVA(
    data = data_ind %>% filter(type == "same", component == "background"),
    dv = .(specific_recog),
    wid = .(code_s1), 
    within = .(valence),
    between = .(condition)
)
print(anova_2way_spe_recog_bgd)
```
A 2 (condition: sleep, wake) × 2 (valence: negative, neutral) mixed ANOVA on specific recognition of objects showed a main effect of valence, *F*(1, 21) = 37.72, *p* < .001, $\eta^2_{p}$ = .168. However, there was no significant condition by valence interaction effect on the specific recognition of objects, *F*(1, 21) = 0.63, *p* = .435, $\eta^2_{p}$ = .003. And a 2 (condition: sleep, wake) × 2 (valence: negative, neutral) mixed ANOVA on specific recognition of background also showed a condition by valence interaction, *F*(1, 21) = 23.48, *p* < .001, $\eta^2_{p}$ = .190. The result of specific recognition of objects is shown in the subplot B above.

## Discussion

### Summary of Replication Attempt

The current replication study did not find a condition (sleep vs. wake) by valence (neutral vs. negative) on general or specific recognition of objects. The main effect of valence on the general recognition of backgrounds showed that backgrounds in a neutral scene were better recognized than those in a negative scene, regardless of the sleep condition. Taken together, the current study failed to replicate the original result.

### Commentary

The current study has a small sameple size (sleep condition: *n* = 14; wake condition: *n* = 9), and thus may have small statistical power to detect the target effect. However, compared to the original effect size of the condition by valence interaction on general recognition of objects ($\eta^2_{p}$ = .20), the current effect size is still very small ($\eta^2_{p}$ = .025). Even with the planned sample size (*n* = 48), it would only have 45% power to detect such an interaction effect. 

It should also be noted that the variance of the general recognition is also very large in both sleep conditions (see plots below). There are individuals who got 0 general recognition rate, and there are also individuals who got more than 80% general recognition rate. The large individual differences in the task performance (noise) made it more difficult to understand of the effect of interest.

```{r}
ggplot(data_summary , aes(x = condition, y = mean_gen_recog, fill = component))+
  geom_bar(color = "black", stat = "identity", position=position_dodge(), width=0.5) + 
  geom_errorbar(aes(ymin=mean_gen_recog - ci_gen_recog, ymax=mean_gen_recog + ci_gen_recog), position=position_dodge(.5), width=.2, size=1) +
  geom_point(data = data_ind, aes(x = condition, y = general_recog, fill = component), position=position_dodge(.5), color = "darkred", alpha = .5)+
  facet_grid(. ~ valence)+
  labs(title = "General recognition")+
  scale_fill_grey()+ ggthemes::theme_few()
```


